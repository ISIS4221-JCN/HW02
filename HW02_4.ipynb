{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW02 - Mourning Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>lang</th>\n",
       "      <th>emoticon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hoy uno de mis tíos falleció por COVID-19, no ...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hoy falleció mi abuela. Y eso, sin velorio, si...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Muere el primer médico en activo por Covid-19 ...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Historia de una victoria. Historia de los últi...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He visto dos personas a las que les tomé cariñ...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       tag lang  emoticon\n",
       "0  Hoy uno de mis tíos falleció por COVID-19, no ...  mourning   es         0\n",
       "1  Hoy falleció mi abuela. Y eso, sin velorio, si...  mourning   es         0\n",
       "2  Muere el primer médico en activo por Covid-19 ...  mourning   es         0\n",
       "3  Historia de una victoria. Historia de los últi...  mourning   es         1\n",
       "4  He visto dos personas a las que les tomé cariñ...  mourning   es         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk corpus\n",
    "#nltk.download()\n",
    "\n",
    "# Import fnmourning dataset\n",
    "raw_df = pd.read_csv('./data/fnmourning.csv', sep = ',')\n",
    "\n",
    "# Glimpse to raw dataset\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace string tags with binary tags\n",
    "raw_df.tag = raw_df.tag.replace('mourning',1)\n",
    "raw_df.tag = raw_df.tag.replace('no mourning',0)\n",
    "\n",
    "# Split dataset by language\n",
    "es_df = raw_df[(raw_df.lang == \"es\")]\n",
    "en_df = raw_df[(raw_df.lang == \"en\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mourning Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet tokenizer \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_tweet(tokenizer, tweet, stop_words):\n",
    "    \"\"\" Applies standard pre-processing to given tweet.\n",
    "    \n",
    "    Args:\n",
    "        text (str): tweet to preprocess.\n",
    "        language (str): languague of the tweet.\n",
    "        stop_words (str): list of words to be removed.\n",
    "    \n",
    "    Returns:\n",
    "        list: preprocessed text.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove punctuations and convert characters to lower case\n",
    "    tweet_nopunct = \"\".join([char.lower() for char in tweet if char not in string.punctuation]) \n",
    "    \n",
    "    # Tokenize words\n",
    "    tk_tweet = tokenizer.tokenize(tweet_nopunct)\n",
    "    \n",
    "    # Remove stop words\n",
    "    relevant_words=[]\n",
    "    for word in tk_tweet:\n",
    "        if word not in stop_words:\n",
    "            relevant_words.append(word)\n",
    "    \n",
    "    #print(relevant_words)\n",
    "    \n",
    "    # Returns processed text\n",
    "    return relevant_words\n",
    "    \n",
    "# Creates tweet tokenizer\n",
    "tt = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "# Tokenize tweets in ES dataset\n",
    "es_stop_words = stopwords.words('spanish')\n",
    "es_tweets = []\n",
    "for tweet in es_df.text:\n",
    "    # Tokenize tweets\n",
    "    es_tweets.append(process_tweet(tt, tweet, es_stop_words))\n",
    "\n",
    "# Tokenize tweets in EN dataset\n",
    "en_stop_words = stopwords.words('english')\n",
    "en_tweets = []\n",
    "for tweet in en_df.text:\n",
    "    # Tokenize tweets\n",
    "    en_tweets.append(process_tweet(tt, tweet, en_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES Dictionary:\n",
      "Dictionary(38653 unique tokens: ['19', 'conciencia', 'condolencias', 'covid', 'cuenta']...)\n",
      "\n",
      "EN Dictionary:\n",
      "Dictionary(32643 unique tokens: ['19', 'away', 'burton', 'clinicians', 'continue']...)\n"
     ]
    }
   ],
   "source": [
    "# Build dictionaries\n",
    "from gensim import corpora\n",
    "\n",
    "# Convert tweets to ES dictionary\n",
    "es_dict = corpora.Dictionary(es_tweets)\n",
    "\n",
    "# Convert tweets to ES dictionary\n",
    "en_dict = corpora.Dictionary(en_tweets)\n",
    "\n",
    "# Glimpse to dictionaries\n",
    "print('ES Dictionary:')\n",
    "print(es_dict)\n",
    "print('')\n",
    "print('EN Dictionary:')\n",
    "print(en_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BOW Representation of each tweet\n",
    "es_corpus = []\n",
    "en_corpus = []\n",
    "\n",
    "# ES tweets\n",
    "for tweet in es_tweets:\n",
    "    es_corpus.append(es_dict.doc2bow(tweet))\n",
    "    \n",
    "# EN tweets\n",
    "for tweet in en_tweets:\n",
    "    en_corpus.append(en_dict.doc2bow(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicons(dictionary, tweet_corpus, tags):\n",
    "    \"\"\" Create lexicons for mourning tweets.\n",
    "    \n",
    "    Args:\n",
    "        dictionary (gensim.corpora.dictionary.Dictionary): Dictionary for the es_.\n",
    "        tweet_corpus (gensim.corpora.mmcorpus.MmCorpus): Tweet corpus with tweets in BOW Model.\n",
    "        tags (numpy.array): Array with mourning tags (1=Mourning, 0=No mourning).\n",
    "    \n",
    "    Returns:\n",
    "        pos (numpy.array): Negative score of each term.\n",
    "        neg (numpy.array): Positive score of each term.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create pos/neg arrays\n",
    "    pos_count = np.zeros(len(dictionary))\n",
    "    neg_count = np.zeros(len(dictionary)) \n",
    "    \n",
    "    # Tweet corpus loop\n",
    "    for i, tweet in enumerate(tweet_corpus):\n",
    "        \n",
    "        # Term loop\n",
    "        for term in tweet:\n",
    "            \n",
    "            # Add to pos/neg array by tag\n",
    "            if tags[i]:\n",
    "                pos_count[term[0]] += term[1]\n",
    "            else:\n",
    "                neg_count[term[0]] += term[1]\n",
    "\n",
    "    # Convert counts to score (scaled likelihood)\n",
    "    p_w = (pos_count+neg_count)/sum(pos_count+neg_count)\n",
    "    \n",
    "    #pos_score = (pos_count/(pos_count+neg_count))#/p_w\n",
    "    #neg_score = (neg_count/(pos_count+neg_count))#/p_w\n",
    "    pos_score = pos_count/sum(pos_count)\n",
    "    neg_score = neg_count/sum(neg_count)\n",
    "\n",
    "    # Return pos/neg score of terms\n",
    "    return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve scores for each language\n",
    "es_pos, es_neg = create_lexicons(es_dict, es_corpus, es_df.tag.values)\n",
    "en_pos, en_neg = create_lexicons(en_dict, en_corpus, en_df.tag.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dataframe with terms\n",
    "es_lexicons = pd.DataFrame(list(es_dict.token2id.items()),columns = ['Term','Id'])\n",
    "en_lexicons = pd.DataFrame(list(en_dict.token2id.items()),columns = ['Term','Id'])\n",
    "\n",
    "# Add pos/neg Scores\n",
    "es_lexicons['PosScore'] = es_pos\n",
    "es_lexicons['NegScore'] = es_neg\n",
    "#es_lexicons['S. O.'] = es_so\n",
    "\n",
    "en_lexicons['PosScore'] = en_pos\n",
    "en_lexicons['NegScore'] = en_neg\n",
    "#en_lexicons['S. O.'] = en_so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Id</th>\n",
       "      <th>PosScore</th>\n",
       "      <th>NegScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>condolencias</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013976</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>qepd</td>\n",
       "      <td>84</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>pesar</td>\n",
       "      <td>364</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010335</td>\n",
       "      <td>0.018806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010202</td>\n",
       "      <td>0.017333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>pena</td>\n",
       "      <td>538</td>\n",
       "      <td>0.009469</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cuarentena</td>\n",
       "      <td>21</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.012950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>familia</td>\n",
       "      <td>36</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.001143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>coronavirus</td>\n",
       "      <td>72</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>0.015955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>dolor</td>\n",
       "      <td>635</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>pandemia</td>\n",
       "      <td>142</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.004042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>🙏</td>\n",
       "      <td>67</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>luto</td>\n",
       "      <td>829</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.004242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>dep</td>\n",
       "      <td>466</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>paz</td>\n",
       "      <td>61</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.001826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>pésame</td>\n",
       "      <td>47</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>si</td>\n",
       "      <td>279</td>\n",
       "      <td>0.004041</td>\n",
       "      <td>0.005998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hoy</td>\n",
       "      <td>11</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.003017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>q</td>\n",
       "      <td>2437</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>0.002863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>fallecimiento</td>\n",
       "      <td>489</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>amigos</td>\n",
       "      <td>522</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>falleció</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.008790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20466</th>\n",
       "      <td>condolences</td>\n",
       "      <td>20466</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>familiares</td>\n",
       "      <td>111</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.000389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>gente</td>\n",
       "      <td>209</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>️</td>\n",
       "      <td>199</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>0.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ser</td>\n",
       "      <td>85</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.002451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>vida</td>\n",
       "      <td>311</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.001390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>día</td>\n",
       "      <td>132</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.002651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>dios</td>\n",
       "      <td>663</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.001061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Term     Id  PosScore  NegScore\n",
       "2       condolencias      2  0.013976  0.000247\n",
       "84              qepd     84  0.013887  0.000589\n",
       "364            pesar    364  0.011567  0.000059\n",
       "3              covid      3  0.010335  0.018806\n",
       "0                 19      0  0.010202  0.017333\n",
       "538             pena    538  0.009469  0.000330\n",
       "21        cuarentena     21  0.009269  0.012950\n",
       "36           familia     36  0.009247  0.001143\n",
       "72       coronavirus     72  0.007515  0.015955\n",
       "635            dolor    635  0.006894  0.000306\n",
       "142         pandemia    142  0.006827  0.004042\n",
       "67                 🙏     67  0.005872  0.000789\n",
       "829             luto    829  0.005573  0.004242\n",
       "466              dep    466  0.005162  0.000966\n",
       "61               paz     61  0.004629  0.001826\n",
       "47            pésame     47  0.004296  0.000318\n",
       "279               si    279  0.004041  0.005998\n",
       "11               hoy     11  0.003730  0.003017\n",
       "2437               q   2437  0.003641  0.002863\n",
       "489    fallecimiento    489  0.003508  0.000177\n",
       "522           amigos    522  0.003452  0.000507\n",
       "8           falleció      8  0.003430  0.008790\n",
       "20466    condolences  20466  0.003053  0.000012\n",
       "111       familiares    111  0.002975  0.000389\n",
       "209            gente    209  0.002853  0.002039\n",
       "199                ️    199  0.002742  0.003535\n",
       "85               ser     85  0.002675  0.002451\n",
       "311             vida    311  0.002609  0.001390\n",
       "132              día    132  0.002587  0.002651\n",
       "663             dios    663  0.002553  0.001061"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print top 30 ES Lexicons for mourning\n",
    "pd.set_option(\"max_rows\", 30)\n",
    "es_lexicons.sort_values(by='PosScore', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Id</th>\n",
       "      <th>PosScore</th>\n",
       "      <th>NegScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid</td>\n",
       "      <td>5</td>\n",
       "      <td>0.024279</td>\n",
       "      <td>0.031610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rip</td>\n",
       "      <td>13</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.002825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020889</td>\n",
       "      <td>0.029534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>’</td>\n",
       "      <td>71</td>\n",
       "      <td>0.013228</td>\n",
       "      <td>0.013723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>coronavirus</td>\n",
       "      <td>29</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.020277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>peace</td>\n",
       "      <td>100</td>\n",
       "      <td>0.010016</td>\n",
       "      <td>0.003350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>rest</td>\n",
       "      <td>101</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>condolences</td>\n",
       "      <td>56</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>🙏</td>\n",
       "      <td>241</td>\n",
       "      <td>0.009409</td>\n",
       "      <td>0.001675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>family</td>\n",
       "      <td>183</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.001362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>sorry</td>\n",
       "      <td>1188</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>lost</td>\n",
       "      <td>79</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>loss</td>\n",
       "      <td>363</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>died</td>\n",
       "      <td>107</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.003607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>may</td>\n",
       "      <td>186</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.001943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>amp</td>\n",
       "      <td>51</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.004924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>people</td>\n",
       "      <td>437</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>0.004913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>prayers</td>\n",
       "      <td>189</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.002155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>one</td>\n",
       "      <td>202</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.002691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>️</td>\n",
       "      <td>266</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.002468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>pandemic</td>\n",
       "      <td>204</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.003082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>httpstco</td>\n",
       "      <td>164</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.004210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>sad</td>\n",
       "      <td>172</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>away</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>due</td>\n",
       "      <td>94</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.001228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>💔</td>\n",
       "      <td>103</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>passed</td>\n",
       "      <td>99</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>us</td>\n",
       "      <td>17</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.003584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>loved</td>\n",
       "      <td>116</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>families</td>\n",
       "      <td>256</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Term    Id  PosScore  NegScore\n",
       "5           covid     5  0.024279  0.031610\n",
       "13            rip    13  0.021400  0.002825\n",
       "0              19     0  0.020889  0.029534\n",
       "71              ’    71  0.013228  0.013723\n",
       "29    coronavirus    29  0.012050  0.020277\n",
       "100         peace   100  0.010016  0.003350\n",
       "101          rest   101  0.009540  0.000837\n",
       "56    condolences    56  0.009457  0.000793\n",
       "241             🙏   241  0.009409  0.001675\n",
       "183        family   183  0.008922  0.001362\n",
       "1188        sorry  1188  0.007863  0.000123\n",
       "79           lost    79  0.007661  0.000837\n",
       "363          loss   363  0.007042  0.000335\n",
       "107          died   107  0.006923  0.003607\n",
       "186           may   186  0.006864  0.001943\n",
       "51            amp    51  0.006114  0.004924\n",
       "437        people   437  0.004556  0.004913\n",
       "189       prayers   189  0.004461  0.002155\n",
       "202           one   202  0.004354  0.002691\n",
       "266             ️   266  0.003961  0.002468\n",
       "204      pandemic   204  0.003890  0.003082\n",
       "164      httpstco   164  0.003854  0.004210\n",
       "172           sad   172  0.003771  0.000380\n",
       "1            away     1  0.003604  0.000793\n",
       "94            due    94  0.003509  0.001228\n",
       "103             💔   103  0.003450  0.000223\n",
       "99         passed    99  0.003366  0.000335\n",
       "17             us    17  0.003295  0.003584\n",
       "116         loved   116  0.003271  0.000491\n",
       "256      families   256  0.003176  0.000770"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print top 30 EN Lexicons for mourning\n",
    "en_lexicons.sort_values(by='PosScore', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_representation(dictionary, corpus, remove_emojis=False, emoji_list=None):\n",
    "    \"\"\" Build BOW Model Matrix representation for tweets in corpus.\n",
    "    \n",
    "    Args:\n",
    "        dictionary (gensim.corpora.dictionary.Dictionary): Dictionary for the tweet corpus.\n",
    "        tweet_corpus (gensim.corpora.mmcorpus.MmCorpus): Tweet corpus with tweets in BOW Model.\n",
    "        remove_emojis (boolean): Whether to remove or not to remove emojis from dictionary.\n",
    "        emoji_list (list): Emojis to be removed from the dictionary (only used if remove_emojis = True)\n",
    "    \n",
    "    Returns:\n",
    "        X (numpy.array): Boolean BOW Model Matrix for tweets in corpus.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Deep copy dictionary\n",
    "    aux_dict = copy.deepcopy(dictionary)\n",
    "    \n",
    "    # Empty Matrix with BOW Model for each corpus\n",
    "    X = np.zeros((len(corpus),len(dictionary)), dtype = np.bool_)\n",
    "    \n",
    "    # Fill Input Matrix\n",
    "    for i, tweet in enumerate(corpus):\n",
    "        for term in tweet:\n",
    "            X[i][term[0]] = 1\n",
    "            \n",
    "    # Remove emojis\n",
    "    if remove_emojis:\n",
    "        # Get Emoji IDs for Dictionary\n",
    "        emoji_ids = []\n",
    "        for emoji in emoji_list:\n",
    "            try:\n",
    "                emoji_ids.append(dictionary.token2id[emoji])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Delete Emoji Columns on X Matrix\n",
    "        X = np.delete(X, emoji_ids, 1)            \n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Boolean BOW Model WITH Emojis\n",
    "X_es = build_feature_representation(es_dict, es_corpus)\n",
    "X_en = build_feature_representation(en_dict, en_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import emoji list (Download emoji Module)\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "# Merge ES and EN Emoji List\n",
    "emoji_list = list({**UNICODE_EMOJI['en'], **UNICODE_EMOJI['es']}.keys())\n",
    "\n",
    "# Build Boolean BOW Model WITHOUT Emojis\n",
    "X_es_no_emojis = build_feature_representation(es_dict, es_corpus, remove_emojis=True, emoji_list=emoji_list)\n",
    "X_en_no_emojis = build_feature_representation(es_dict, es_corpus, remove_emojis=True, emoji_list=emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13168, 38653)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13168, 38148)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_es_no_emojis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
