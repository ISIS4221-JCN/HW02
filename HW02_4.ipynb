{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW02 - Mourning Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>lang</th>\n",
       "      <th>emoticon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hoy uno de mis tíos falleció por COVID-19, no ...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hoy falleció mi abuela. Y eso, sin velorio, si...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Muere el primer médico en activo por Covid-19 ...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Historia de una victoria. Historia de los últi...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He visto dos personas a las que les tomé cariñ...</td>\n",
       "      <td>mourning</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       tag lang  emoticon\n",
       "0  Hoy uno de mis tíos falleció por COVID-19, no ...  mourning   es         0\n",
       "1  Hoy falleció mi abuela. Y eso, sin velorio, si...  mourning   es         0\n",
       "2  Muere el primer médico en activo por Covid-19 ...  mourning   es         0\n",
       "3  Historia de una victoria. Historia de los últi...  mourning   es         1\n",
       "4  He visto dos personas a las que les tomé cariñ...  mourning   es         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk corpus\n",
    "#nltk.download()\n",
    "\n",
    "# Import fnmourning dataset\n",
    "raw_df = pd.read_csv('./data/fnmourning.csv', sep = ',')\n",
    "\n",
    "# Glimpse to raw dataset\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace string tags with binary tags\n",
    "raw_df.tag = raw_df.tag.replace('mourning',1)\n",
    "raw_df.tag = raw_df.tag.replace('no mourning',0)\n",
    "\n",
    "# Split dataset by language\n",
    "es_df = raw_df[(raw_df.lang == \"es\")]\n",
    "en_df = raw_df[(raw_df.lang == \"en\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet tokenizer \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_tweet(tokenizer, tweet, stop_words):\n",
    "    \"\"\" Applies standard pre-processing to given tweet.\n",
    "    \n",
    "    Args:\n",
    "        text (str): tweet to preprocess.\n",
    "        language (str): languague of the tweet.\n",
    "        stop_words (str): list of words to be removed.\n",
    "    \n",
    "    Returns:\n",
    "        list: preprocessed text.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove punctuations and convert characters to lower case\n",
    "    tweet_nopunct = \"\".join([char.lower() for char in tweet if char not in string.punctuation]) \n",
    "    \n",
    "    # Tokenize words\n",
    "    tk_tweet = tokenizer.tokenize(tweet_nopunct)\n",
    "    \n",
    "    # Remove stop words\n",
    "    relevant_words=[]\n",
    "    for word in tk_tweet:\n",
    "        if word not in stop_words:\n",
    "            relevant_words.append(word)\n",
    "    \n",
    "    #print(relevant_words)\n",
    "    \n",
    "    # Returns processed text\n",
    "    return relevant_words\n",
    "    \n",
    "# Creates tweet tokenizer\n",
    "tt = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "# Tokenize tweets in ES dataset\n",
    "es_stop_words = stopwords.words('spanish')\n",
    "es_tweets = []\n",
    "for tweet in es_df.text:\n",
    "    # Tokenize tweets\n",
    "    es_tweets.append(process_tweet(tt, tweet, es_stop_words))\n",
    "\n",
    "# Tokenize tweets in EN dataset\n",
    "en_stop_words = stopwords.words('english')\n",
    "en_tweets = []\n",
    "for tweet in en_df.text:\n",
    "    # Tokenize tweets\n",
    "    en_tweets.append(process_tweet(tt, tweet, en_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES Dictionary:\n",
      "Dictionary(38653 unique tokens: ['19', 'conciencia', 'condolencias', 'covid', 'cuenta']...)\n",
      "\n",
      "EN Dictionary:\n",
      "Dictionary(32643 unique tokens: ['19', 'away', 'burton', 'clinicians', 'continue']...)\n"
     ]
    }
   ],
   "source": [
    "# Build dictionaries\n",
    "from gensim import corpora\n",
    "\n",
    "# Convert tweets to ES dictionary\n",
    "es_dict = corpora.Dictionary(es_tweets)\n",
    "\n",
    "# Convert tweets to ES dictionary\n",
    "en_dict = corpora.Dictionary(en_tweets)\n",
    "\n",
    "# Glimpse to dictionaries\n",
    "print('ES Dictionary:')\n",
    "print(es_dict)\n",
    "print('')\n",
    "print('EN Dictionary:')\n",
    "print(en_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BOW Representation of each tweet\n",
    "es_corpus = []\n",
    "en_corpus = []\n",
    "\n",
    "# ES tweets\n",
    "for tweet in es_tweets:\n",
    "    es_corpus.append(es_dict.doc2bow(tweet))\n",
    "    \n",
    "# EN tweets\n",
    "for tweet in en_tweets:\n",
    "    en_corpus.append(en_dict.doc2bow(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicons(dictionary, tweet_corpus, tags):\n",
    "    \"\"\" Create lexicons for mourning tweets.\n",
    "    \n",
    "    Args:\n",
    "        dictionary (gensim.corpora.dictionary.Dictionary): Dictionary for the es_.\n",
    "        tweet_corpus (gensim.corpora.mmcorpus.MmCorpus): Tweet corpus with tweets in BOW Model.\n",
    "        tags (numpy.array): Array with mourning tags (1=Mourning, 0=No mourning).\n",
    "    \n",
    "    Returns:\n",
    "        pos (numpy.array): Negative score of each term.\n",
    "        neg (numpy.array): Positive score of each term.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create pos/neg arrays\n",
    "    pos = np.zeros(len(dictionary))\n",
    "    neg = np.zeros(len(dictionary)) \n",
    "    \n",
    "    # Tweet corpus loop\n",
    "    for i, tweet in enumerate(tweet_corpus):\n",
    "        \n",
    "        # Term loop\n",
    "        for term in tweet:\n",
    "            \n",
    "            # Add to pos/neg array by tag\n",
    "            if tags[i]:\n",
    "                pos[term[0]] += term[1]\n",
    "            else:\n",
    "                neg[term[0]] += term[1]\n",
    "\n",
    "    # Convert counts to score\n",
    "    pos = pos/sum(pos)\n",
    "    neg = neg/sum(neg)\n",
    "    overall = pos - neg\n",
    "\n",
    "    # Return pos/neg score of terms\n",
    "    return pos, neg, overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve scores for each language\n",
    "es_pos, es_neg, es_overall = create_lexicons(es_dict, es_corpus, es_df.tag.values)\n",
    "en_pos, en_neg, en_overall = create_lexicons(en_dict, en_corpus, en_df.tag.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dataframe with terms\n",
    "es_lexicons = pd.DataFrame(list(es_dict.token2id.items()),columns = ['Term','Id'])\n",
    "en_lexicons = pd.DataFrame(list(en_dict.token2id.items()),columns = ['Term','Id'])\n",
    "\n",
    "# Add pos/neg Scores\n",
    "es_lexicons['PosScore'] = es_pos\n",
    "es_lexicons['NegScore'] = es_neg\n",
    "es_lexicons['Score'] = es_overall\n",
    "\n",
    "en_lexicons['PosScore'] = en_pos\n",
    "en_lexicons['NegScore'] = en_neg\n",
    "en_lexicons['Score'] = en_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Id</th>\n",
       "      <th>PosScore</th>\n",
       "      <th>NegScore</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>condolencias</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013976</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.013729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>qepd</td>\n",
       "      <td>84</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.013298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>pesar</td>\n",
       "      <td>364</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.011508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>pena</td>\n",
       "      <td>538</td>\n",
       "      <td>0.009469</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.009139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>familia</td>\n",
       "      <td>36</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.008104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>dolor</td>\n",
       "      <td>635</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.006587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>🙏</td>\n",
       "      <td>67</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.005083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>dep</td>\n",
       "      <td>466</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.004196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>pésame</td>\n",
       "      <td>47</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.003978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>fallecimiento</td>\n",
       "      <td>489</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.003331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20466</th>\n",
       "      <td>condolences</td>\n",
       "      <td>20466</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.003041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>amigos</td>\n",
       "      <td>522</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.002946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>paz</td>\n",
       "      <td>61</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.002803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>pandemia</td>\n",
       "      <td>142</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.002785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>familiares</td>\n",
       "      <td>111</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.002586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>descansa</td>\n",
       "      <td>286</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.002315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>abrazo</td>\n",
       "      <td>30</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.002214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20565</th>\n",
       "      <td>pokzbo</td>\n",
       "      <td>20565</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>sinceras</td>\n",
       "      <td>744</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>peña</td>\n",
       "      <td>4264</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>dios</td>\n",
       "      <td>663</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.001493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gran</td>\n",
       "      <td>37</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.001468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>toda</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.001455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>luto</td>\n",
       "      <td>829</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>amigo</td>\n",
       "      <td>238</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.001308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>tan</td>\n",
       "      <td>957</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.001304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>vida</td>\n",
       "      <td>311</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.001218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>vale</td>\n",
       "      <td>2346</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.001215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>edelamadrid</td>\n",
       "      <td>4036</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>siempre</td>\n",
       "      <td>118</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.001188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Term     Id  PosScore  NegScore     Score\n",
       "2       condolencias      2  0.013976  0.000247  0.013729\n",
       "84              qepd     84  0.013887  0.000589  0.013298\n",
       "364            pesar    364  0.011567  0.000059  0.011508\n",
       "538             pena    538  0.009469  0.000330  0.009139\n",
       "36           familia     36  0.009247  0.001143  0.008104\n",
       "635            dolor    635  0.006894  0.000306  0.006587\n",
       "67                 🙏     67  0.005872  0.000789  0.005083\n",
       "466              dep    466  0.005162  0.000966  0.004196\n",
       "47            pésame     47  0.004296  0.000318  0.003978\n",
       "489    fallecimiento    489  0.003508  0.000177  0.003331\n",
       "20466    condolences  20466  0.003053  0.000012  0.003041\n",
       "522           amigos    522  0.003452  0.000507  0.002946\n",
       "61               paz     61  0.004629  0.001826  0.002803\n",
       "142         pandemia    142  0.006827  0.004042  0.002785\n",
       "111       familiares    111  0.002975  0.000389  0.002586\n",
       "286         descansa    286  0.002398  0.000082  0.002315\n",
       "30            abrazo     30  0.002520  0.000306  0.002214\n",
       "20565         pokzbo  20565  0.001754  0.000012  0.001742\n",
       "744         sinceras    744  0.001754  0.000012  0.001742\n",
       "4264            peña   4264  0.001532  0.000012  0.001520\n",
       "663             dios    663  0.002553  0.001061  0.001493\n",
       "37              gran     37  0.002187  0.000719  0.001468\n",
       "1147            toda   1147  0.002209  0.000754  0.001455\n",
       "829             luto    829  0.005573  0.004242  0.001331\n",
       "238            amigo    238  0.001721  0.000412  0.001308\n",
       "957              tan    957  0.002364  0.001061  0.001304\n",
       "311             vida    311  0.002609  0.001390  0.001218\n",
       "2346            vale   2346  0.001510  0.000295  0.001215\n",
       "4036     edelamadrid   4036  0.001199  0.000000  0.001199\n",
       "118          siempre    118  0.002142  0.000954  0.001188"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print top 30 ES Lexicons for mourning\n",
    "pd.set_option(\"max_rows\", 30)\n",
    "es_lexicons.sort_values(by='Score', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Id</th>\n",
       "      <th>PosScore</th>\n",
       "      <th>NegScore</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rip</td>\n",
       "      <td>13</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.018575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>rest</td>\n",
       "      <td>101</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.008703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>condolences</td>\n",
       "      <td>56</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.008664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>sorry</td>\n",
       "      <td>1188</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.007740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>🙏</td>\n",
       "      <td>241</td>\n",
       "      <td>0.009409</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.007735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>family</td>\n",
       "      <td>183</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.007560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>lost</td>\n",
       "      <td>79</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.006823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>loss</td>\n",
       "      <td>363</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.006707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>peace</td>\n",
       "      <td>100</td>\n",
       "      <td>0.010016</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.006666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>may</td>\n",
       "      <td>186</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.004921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>sad</td>\n",
       "      <td>172</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.003391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>died</td>\n",
       "      <td>107</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.003317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>💔</td>\n",
       "      <td>103</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.003226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>passed</td>\n",
       "      <td>99</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.003032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>away</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.002812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>loved</td>\n",
       "      <td>116</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.002780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>roy</td>\n",
       "      <td>7994</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>families</td>\n",
       "      <td>256</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.002406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>prayers</td>\n",
       "      <td>189</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>due</td>\n",
       "      <td>94</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.002281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>friends</td>\n",
       "      <td>184</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.002178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>ones</td>\n",
       "      <td>203</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>❤</td>\n",
       "      <td>520</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.002121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>soul</td>\n",
       "      <td>191</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.002070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>hear</td>\n",
       "      <td>878</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.002023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>😢</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lives</td>\n",
       "      <td>11</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.001713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>one</td>\n",
       "      <td>202</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.001663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>death</td>\n",
       "      <td>57</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.001629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>deepest</td>\n",
       "      <td>197</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Term    Id  PosScore  NegScore     Score\n",
       "13            rip    13  0.021400  0.002825  0.018575\n",
       "101          rest   101  0.009540  0.000837  0.008703\n",
       "56    condolences    56  0.009457  0.000793  0.008664\n",
       "1188        sorry  1188  0.007863  0.000123  0.007740\n",
       "241             🙏   241  0.009409  0.001675  0.007735\n",
       "183        family   183  0.008922  0.001362  0.007560\n",
       "79           lost    79  0.007661  0.000837  0.006823\n",
       "363          loss   363  0.007042  0.000335  0.006707\n",
       "100         peace   100  0.010016  0.003350  0.006666\n",
       "186           may   186  0.006864  0.001943  0.004921\n",
       "172           sad   172  0.003771  0.000380  0.003391\n",
       "107          died   107  0.006923  0.003607  0.003317\n",
       "103             💔   103  0.003450  0.000223  0.003226\n",
       "99         passed    99  0.003366  0.000335  0.003032\n",
       "1            away     1  0.003604  0.000793  0.002812\n",
       "116         loved   116  0.003271  0.000491  0.002780\n",
       "7994          roy  7994  0.002605  0.000000  0.002605\n",
       "256      families   256  0.003176  0.000770  0.002406\n",
       "189       prayers   189  0.004461  0.002155  0.002306\n",
       "94            due    94  0.003509  0.001228  0.002281\n",
       "184       friends   184  0.002914  0.000737  0.002178\n",
       "203          ones   203  0.002546  0.000402  0.002144\n",
       "520             ❤   520  0.003093  0.000971  0.002121\n",
       "191          soul   191  0.002248  0.000179  0.002070\n",
       "878          hear   878  0.002403  0.000380  0.002023\n",
       "128             😢   128  0.002022  0.000156  0.001866\n",
       "11          lives    11  0.002439  0.000726  0.001713\n",
       "202           one   202  0.004354  0.002691  0.001663\n",
       "57          death    57  0.003081  0.001452  0.001629\n",
       "197       deepest   197  0.001582  0.000011  0.001571"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print top 30 EN Lexicons for mourning\n",
    "en_lexicons.sort_values(by='Score', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
