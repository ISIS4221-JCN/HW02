{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW02: N-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports requiered libraries\n",
    "import os\n",
    "\n",
    "# Impports minidom for XML file processing\n",
    "from lxml import etree\n",
    "\n",
    "# Imports NLTK for tokenizing\n",
    "import nltk\n",
    "\n",
    "# Imports counter for term frequency\n",
    "from collections import Counter\n",
    "\n",
    "# Imports time function for performance measurements\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets prefix where the dataset is\n",
    "NEWS_PREFIX = \"./resources/news/\"\n",
    "\n",
    "# Sets the news consolidate file\n",
    "NEWS_CONSOLIDATE = \"./resources/news_consolidate.txt\"\n",
    "\n",
    "# Counts the number of processed files\n",
    "n = 0\n",
    "\n",
    "# Sets the initial time\n",
    "initial_time = time()\n",
    "\n",
    "# Opens the consolidate file\n",
    "with open(NEWS_CONSOLIDATE, 'wb') as consolidate:\n",
    "\n",
    "    # Iterates over every folder in dataset\n",
    "    for folder in os.listdir(NEWS_PREFIX):\n",
    "        \n",
    "        # Iterates over every file in subfolders\n",
    "        for filename in os.listdir(NEWS_PREFIX + folder):\n",
    "            \n",
    "            # Opens file\n",
    "            file = open(NEWS_PREFIX + folder + '/' + filename, 'rb').read()\n",
    "            \n",
    "            # Writes the file content into consolidate\n",
    "            consolidate.write(file)\n",
    "            \n",
    "            # Increments number of processed files\n",
    "            n += 1\n",
    "            \n",
    "# Calculates execution time\n",
    "final_time = time() - initial_time\n",
    "\n",
    "# Prints relevant information\n",
    "print(\"Processed: {} files in {:4.2f} seconds\".format(n, final_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets prefix where the dataset is\n",
    "BLOGS_PREFIX = \"./resources/blogs/\"\n",
    "\n",
    "# Sets the blog consolidate file\n",
    "BLOGS_CONSOLIDATE = \"./resources/blogs_consolidate.txt\"\n",
    "\n",
    "# Counts the number of processed files\n",
    "n = 0\n",
    "\n",
    "# Sets the initial time\n",
    "initial_time = time()\n",
    "\n",
    "# Opens consolidate file\n",
    "with open(BLOGS_CONSOLIDATE, 'w') as consolidate:\n",
    "\n",
    "    # Iterates over files in dataset\n",
    "    for filename in os.listdir(BLOGS_PREFIX):\n",
    "\n",
    "        # Opens the file\n",
    "        text = open(BLOGS_PREFIX + filename, 'r', encoding=\"latin-1\").read()\n",
    "\n",
    "        # Parses the file and retrieves the element tree\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        tree = etree.fromstring(text, parser=parser)\n",
    "        \n",
    "        # Writes content to consolidate file\n",
    "        consolidate.write(tree[1].text)\n",
    "        \n",
    "        # Increments number of processed files\n",
    "        n += 1\n",
    "        \n",
    "# Calculates execution time\n",
    "final_time = time() - initial_time\n",
    "\n",
    "# Prints relevant information\n",
    "print(\"Processed: {} files in {:4.2f} seconds\".format(n, final_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \n",
    "    # Imports GENSIM PorterStemmer\n",
    "    from gensim.parsing.porter import PorterStemmer\n",
    "    \n",
    "    # Imports regular expressions substitution\n",
    "    from re import sub, match\n",
    "    \n",
    "    # Tokenizes text into sentences\n",
    "    text = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Creates the stemmer instance\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # List to store processed sentences\n",
    "    processed_text = []\n",
    "    \n",
    "    # Iterates over sentences\n",
    "    for sentence in text:\n",
    "    \n",
    "        # Sets text in lower case\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Replaces numbers\n",
    "        sentence = sub(r'[0-9]+', 'NUM', sentence)\n",
    "        \n",
    "        # Stems text\n",
    "        sentence = stemmer.stem_sentence(sentence)\n",
    "        \n",
    "        # Tokenizes text\n",
    "        sentence = nltk.wordpunct_tokenize(sentence)\n",
    "        \n",
    "        # Removes punctuation marks\n",
    "        sentence = [ token for token in sentence if not match(r'[^\\w\\s]', token) ]\n",
    "        \n",
    "        # Adds tags to sentence\n",
    "        sentence.insert(0, '<s>')\n",
    "        sentence.append(\"</s>\")\n",
    "        \n",
    "        # Appends sentence to list\n",
    "        processed_text.append(sentence)\n",
    "    \n",
    "    # Creates a single list with all tokens\n",
    "    processed_text = [ token for sentence in processed_text for token in sentence ]\n",
    "    \n",
    "    # Counts token frequency\n",
    "    token_freq = dict(Counter(processed_text))\n",
    "    \n",
    "    # Gets the unique tokens from dictionary\n",
    "    unique_tokens = [ token for token in token_freq.keys() if token_freq[token] == 1 ]\n",
    "\n",
    "    # Replaces unique tokens in corpus\n",
    "    for i, token in enumerate(unique_tokens):\n",
    "        processed_text[processed_text.index(token)] = \"<UNK>\"\n",
    "        \n",
    "    # Creates lists to store corpus and current sentence\n",
    "    corpus = []\n",
    "    sentence = []\n",
    "        \n",
    "    # Returns corpus to sentences\n",
    "    for i, token in enumerate(processed_text):\n",
    "        \n",
    "        # Appends token to current sentence\n",
    "        sentence.append(token)\n",
    "        \n",
    "        # Resets sentence if end-of-sentence token is found\n",
    "        if token == \"</s>\":\n",
    "            corpus.append(sentence)\n",
    "            sentence = []\n",
    "            \n",
    "    # Returns corpus \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens both datasets\n",
    "news_text = open(NEWS_CONSOLIDATE, 'rb').read().decode('utf-8', 'ignore')\n",
    "blogs_text = open(BLOGS_CONSOLIDATE, 'rb').read().decode('utf-8', 'ignore')\n",
    "\n",
    "# Process news text files\n",
    "initial_time = time()\n",
    "news_processed_text = process_text(news_text)\n",
    "final_time = time() - initial_time\n",
    "print(\"Processed news text in {:4.2f} seconds\".format(final_time))\n",
    "\n",
    "# Process blogs text files\n",
    "initial_time = time()\n",
    "blogs_processed_text = process_text(blogs_text)\n",
    "final_time = time() - initial_time\n",
    "print(\"Processed blogs text in {:4.2f} seconds\".format(final_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save dataset on disk\n",
    "def save_dataset(data, filename):\n",
    "    \n",
    "    # Imports csv to store data in such format\n",
    "    import csv\n",
    "\n",
    "    # Opens target file to write\n",
    "    with open(filename, 'w') as file:\n",
    "\n",
    "        # Creates a writer for the file\n",
    "        csv_writer = csv.writer(file)\n",
    "\n",
    "        # Stores each sentence in file\n",
    "        for sentence in data:\n",
    "            csv_writer.writerow(sentence)\n",
    "\n",
    "\n",
    "# Defines filenames for processed text\n",
    "news_filename = './resources/20N_7_data.csv'\n",
    "blogs_filename = './resources/BAC_7_data.csv'\n",
    "\n",
    "# Stores processed data\n",
    "save_dataset(news_processed_text, news_filename)\n",
    "save_dataset(blogs_processed_text, blogs_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open the processed datasets\n",
    "def open_dataset(filename):\n",
    "    \n",
    "    # Imports csv to read data in such format\n",
    "    import csv\n",
    "    \n",
    "    # Opens target file to read\n",
    "    with open(filename, 'r') as file:\n",
    "        \n",
    "        # Creates a reader for the file\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        # List to store the corpus of the collection\n",
    "        corpus = []\n",
    "        \n",
    "        # Reads rows in file\n",
    "        for row in csv_reader:\n",
    "            corpus.append(list(row))\n",
    "            \n",
    "    # Returns text corpus\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# Defines filenames for processed text\n",
    "news_filename = './resources/20N_7_data.csv'\n",
    "blogs_filename = './resources/BAC_7_data.csv'\n",
    "\n",
    "# Reads corpus from CSV files\n",
    "news_corpus = open_dataset(news_filename)\n",
    "blogs_corpus = open_dataset(blogs_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports splitter for data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splits news data\n",
    "news_train_data, news_test_data = train_test_split(news_corpus, test_size=0.2)\n",
    "\n",
    "# Splits blog data\n",
    "blogs_train_data, blogs_test_data = train_test_split(blogs_corpus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines news dataset filenames\n",
    "news_train_filename = \"./resources/20N_7_training.csv\"\n",
    "news_test_filename = \"./resources/20N_7_testing.csv\"\n",
    "\n",
    "# Defines blogs dataset filenames\n",
    "blogs_train_filename = \"./resources/BAC_7_training.csv\"\n",
    "blogs_test_filename = \"./resources/BAC_7_testing.csv\"\n",
    "\n",
    "# Saves splitted news dataset\n",
    "save_dataset(news_train_data, news_train_filename)\n",
    "save_dataset(news_test_data, news_test_filename)\n",
    "\n",
    "# Saves splitted blogs dataset\n",
    "save_dataset(blogs_train_data, blogs_train_filename)\n",
    "save_dataset(blogs_test_data, blogs_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open the processed datasets\n",
    "def open_dataset(filename):\n",
    "    \n",
    "    # Imports csv to read data in such format\n",
    "    import csv\n",
    "    \n",
    "    # Opens target file to read\n",
    "    with open(filename, 'r') as file:\n",
    "        \n",
    "        # Creates a reader for the file\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        # List to store the corpus of the collection\n",
    "        corpus = []\n",
    "        \n",
    "        # Reads rows in file\n",
    "        for row in csv_reader:\n",
    "            corpus.append(list(row))\n",
    "            \n",
    "    # Returns text corpus\n",
    "    return corpus\n",
    "\n",
    "# Defines news dataset filenames\n",
    "news_train_filename = \"./resources/20N_7_training.csv\"\n",
    "news_test_filename = \"./resources/20N_7_testing.csv\"\n",
    "\n",
    "# Defines blogs dataset filenames\n",
    "blogs_train_filename = \"./resources/BAC_7_training.csv\"\n",
    "blogs_test_filename = \"./resources/BAC_7_testing.csv\"\n",
    "\n",
    "# Opens train datasets\n",
    "news_train_data = open_dataset(news_train_filename)\n",
    "blogs_train_data = open_dataset(blogs_train_filename)\n",
    "\n",
    "# Opens test datasets\n",
    "news_test_data = open_dataset(news_test_filename)\n",
    "blogs_test_data = open_dataset(blogs_test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count unigrams\n",
    "def count_unigrams(data):\n",
    "    \n",
    "    # Imports utility to create N-Grams\n",
    "    from nltk.util import ngrams\n",
    "    \n",
    "    # Imports default dict from collections\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Creates a defaultdict to store unigrams count\n",
    "    unigrams_count = defaultdict(lambda: 0)\n",
    "    \n",
    "    # Iterates over data\n",
    "    for sentence in data:\n",
    "        \n",
    "        # Iterates over unigrams\n",
    "        for w1, in ngrams(sentence, 1):\n",
    "            \n",
    "            # Increments count for given unigram\n",
    "            unigrams_count[w1] += 1\n",
    "            \n",
    "    # Returns the count\n",
    "    return unigrams_count\n",
    "\n",
    "\n",
    "# Function to count bigrams\n",
    "def count_bigrams(data):\n",
    "    \n",
    "    # Imports utility to create N-Grams\n",
    "    from nltk.util import ngrams\n",
    "    \n",
    "    # Imports default dict from collections\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Defaultdict to store bigrams count\n",
    "    bigrams_count = defaultdict(lambda: 0)\n",
    "    \n",
    "    # Iterates over data\n",
    "    for sentence in data:\n",
    "        \n",
    "        # Iterates over bigrams\n",
    "        for w1, w2 in ngrams(sentence, 2):\n",
    "            \n",
    "            # Increments count for given bigram\n",
    "            bigrams_count[(w1, w2)] += 1\n",
    "    \n",
    "    # Returns the count\n",
    "    return bigrams_count\n",
    "\n",
    "\n",
    "# Function to count trigrams\n",
    "def count_trigrams(data):\n",
    "    \n",
    "    # Imports utility to create N-Grams\n",
    "    from nltk.util import ngrams\n",
    "    \n",
    "    # Imports default dict from collections\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Defaultdict to store bigrams count\n",
    "    trigrams_count = defaultdict(lambda: 0)\n",
    "    \n",
    "    # Iterates over data\n",
    "    for sentence in data:\n",
    "        \n",
    "        # Iterates over trigrams\n",
    "        for w1, w2, w3 in ngrams(sentence, 3):\n",
    "            \n",
    "            # Increments count for given trigrams\n",
    "            trigrams_count[(w1, w2, w3)] += 1\n",
    "    \n",
    "    # Returns the count\n",
    "    return trigrams_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get unigrams with add-1 smoothing\n",
    "def laplace_unigrams(data):\n",
    "    \n",
    "    # Gets the counting for data\n",
    "    unigrams_count = count_unigrams(data)\n",
    "    \n",
    "    # Calculates coefficient for OOV tokens\n",
    "    coeff = 1.0/(sum(unigrams_count.values()) + len(unigrams_count.keys()))\n",
    "    \n",
    "    # Creates a dict for the unigrams\n",
    "    unigrams = {}\n",
    "    \n",
    "    # Calculates probability using Laplace smoothing\n",
    "    for unigram in unigrams_count.keys():\n",
    "        \n",
    "        # Calculates unigram count plus k\n",
    "        upper = unigrams_count[unigram] + 1\n",
    "        \n",
    "        # Calculates word count plus Laplace term\n",
    "        lower = sum(unigrams_count.values()) + len(unigrams_count.keys())\n",
    "        \n",
    "        # Calculates Laplace probability\n",
    "        unigrams[unigram] = upper / lower\n",
    "    \n",
    "    # Returns unigrams model\n",
    "    return unigrams, coeff\n",
    "\n",
    "\n",
    "# Function to get bigrams with add-1 smoothing\n",
    "def laplace_bigrams(data):\n",
    "    \n",
    "    # Gets counts for unigrams\n",
    "    unigrams_count = count_unigrams(data)\n",
    "    \n",
    "    # Gets counts for bigrams\n",
    "    bigrams_count = count_bigrams(data)\n",
    "    \n",
    "    # Calculates OOV coefficient\n",
    "    coeff = 1.0/(sum(unigrams_count.values()) + len(unigrams_count.keys()))\n",
    "    \n",
    "    # Creates a dict to store bigrams\n",
    "    bigrams = {}\n",
    "    \n",
    "    # Iterates over bigrams\n",
    "    for w1, w2 in bigrams_count.keys():\n",
    "        \n",
    "        # Counts occurence of bigram \n",
    "        upper = bigrams_count[(w1, w2)] + 1\n",
    "\n",
    "        # Calculates unigram count plus vocabulary size\n",
    "        lower = unigrams_count[w1] + len(unigrams_count.keys())\n",
    "\n",
    "        # Calculates conditional probability\n",
    "        bigrams[(w1, w2)] = upper / lower\n",
    "    \n",
    "    # Returns bigrams model\n",
    "    return bigrams, coeff\n",
    "\n",
    "\n",
    "# Function to get trigrams with add-1 smoothing\n",
    "def laplace_trigrams(data):\n",
    "    \n",
    "    # Gets counts for trigrams\n",
    "    trigrams_count = count_trigrams(data)\n",
    "    \n",
    "    # Gets counts for bigrams\n",
    "    bigrams_count = count_bigrams(data)\n",
    "    \n",
    "    # Gets count for unigrams\n",
    "    unigrams_count = count_unigrams(data)\n",
    "    \n",
    "    # Calculates OOV coefficient\n",
    "    coeff = 1.0/(sum(bigrams_count.values()) + len(unigrams_count.keys()))\n",
    "    \n",
    "    # Creates a dict to store trigrams\n",
    "    trigrams = {}\n",
    "    \n",
    "    # Iterates over trigrams\n",
    "    for w1, w2, w3 in trigrams_count.keys():\n",
    "        \n",
    "        # Counts occurence of trigram\n",
    "        upper = trigrams_count[(w1, w2, w3)] + 1\n",
    "\n",
    "        # Calculates bigram count plus vocabulary size\n",
    "        lower = bigrams_count[(w1, w2)] + len(unigrams_count.keys())\n",
    "\n",
    "        # Calculates conditional probability\n",
    "        trigrams[(w1, w2, w2)] = upper / lower\n",
    "    \n",
    "    # Returns trigrams model\n",
    "    return trigrams, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 20N unigram model\n",
      "Finished 20N bigram model\n",
      "Finished 20N trigram model\n",
      "Finished BAC unigram model\n",
      "Finished BAC bigram model\n",
      "Finished BAC trigram model\n"
     ]
    }
   ],
   "source": [
    "def store_model(model, coeff, filename, n=1):\n",
    "     \n",
    "    from csv import writer\n",
    "    \n",
    "    with open(filename, 'w') as csv_file:\n",
    "        \n",
    "        csv_writer = writer(csv_file)\n",
    "        \n",
    "        csv_writer.writerow([coeff])\n",
    "        \n",
    "        for key in model.keys():\n",
    "        \n",
    "            if n == 1:\n",
    "            \n",
    "                row = [ key , model[key] ]\n",
    "                \n",
    "                csv_writer.writerow(row)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                row = list(key)\n",
    "                \n",
    "                row.append(model[key])\n",
    "                \n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "                \n",
    "news_uni_dict, news_uni_coeff = laplace_unigrams(news_train_data)\n",
    "news_unigram_model = defaultdict(lambda: news_uni_coeff, news_uni_dict)\n",
    "news_unigram_model_filename = \"./results/ngrams/models/20N_7_unigrams.csv\"\n",
    "store_model(news_uni_dict, news_uni_coeff, news_unigram_model_filename, n=1)\n",
    "print(\"Finished 20N unigram model\")\n",
    "\n",
    "news_bi_dict, news_bi_coeff = laplace_bigrams(news_train_data)\n",
    "news_bigram_model = defaultdict(lambda: news_bi_coeff, news_bi_dict)\n",
    "news_bigram_model_filename = \"./results/ngrams/models/20N_7_bigrams.csv\"\n",
    "store_model(news_bi_dict, news_bi_coeff, news_bigram_model_filename, n=2)\n",
    "print(\"Finished 20N bigram model\")\n",
    "\n",
    "\n",
    "news_tri_dict, news_tri_coeff = laplace_trigrams(news_train_data)\n",
    "news_trigram_model = defaultdict(lambda: news_tri_coeff, news_tri_dict)\n",
    "news_trigram_model_filename = \"./results/ngrams/models/20N_7_trigrams.csv\"\n",
    "store_model(news_tri_dict, news_tri_coeff, news_trigram_model_filename, n=3)\n",
    "print(\"Finished 20N trigram model\")\n",
    "\n",
    "blogs_uni_dict, blogs_uni_coeff = laplace_unigrams(blogs_train_data)\n",
    "blogs_unigram_model = defaultdict(lambda: blogs_uni_coeff, blogs_uni_dict)\n",
    "blogs_unigram_model_filename = \"./results/ngrams/models/BAC_7_unigrams.csv\"\n",
    "store_model(blogs_uni_dict, blogs_uni_coeff, blogs_unigram_model_filename, n=1)\n",
    "print(\"Finished BAC unigram model\")\n",
    "\n",
    "blogs_bi_dict, blogs_bi_coeff = laplace_bigrams(blogs_train_data)\n",
    "blogs_bigram_model = defaultdict(lambda: blogs_bi_coeff, blogs_bi_dict)\n",
    "blogs_bigram_model_filename = \"./results/ngrams/models/BAC_7_bigrams.csv\"\n",
    "store_model(blogs_bi_dict, blogs_bi_coeff, blogs_bigram_model_filename, n=2)\n",
    "print(\"Finished BAC bigram model\")\n",
    "\n",
    "\n",
    "blogs_tri_dict, blogs_tri_coeff = laplace_trigrams(blogs_train_data)\n",
    "blogs_trigram_model = defaultdict(lambda: blogs_tri_coeff, blogs_tri_dict)\n",
    "blogs_trigram_model_filename = \"./results/ngrams/models/BAC_7_trigrams.csv\"\n",
    "store_model(blogs_tri_dict, blogs_tri_coeff, news_trigram_model_filename, n=3)\n",
    "print(\"Finished BAC trigram model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens model\n",
    "def read_model(filename):\n",
    "    \n",
    "    import csv\n",
    "    \n",
    "    coeff = 0\n",
    "    dictionary = {}\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        \n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        row_indx = 0\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            \n",
    "            if row_indx == 0:\n",
    "                coeff = float(row[0])\n",
    "                row_indx += 1\n",
    "                \n",
    "            else:\n",
    "                _len = len(row)\n",
    "                value = float(row[_len-1])\n",
    "                \n",
    "                if _len == 2:\n",
    "                    keys = row[0]\n",
    "                    \n",
    "                else:\n",
    "                    keys = tuple(row[0:_len-1])\n",
    "                    \n",
    "                dictionary[keys] = value\n",
    "                row_indx += 1\n",
    "                \n",
    "    return dictionary, coeff\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Opens news unigram dictionary as example\n",
    "dictionary, coeff = read_model(\"./results/ngrams/models/20N_7_unigrams.csv\")\n",
    "news_unigram_model = defaultdict(lambda: coeff, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates perplexity for a sentence and a model\n",
    "def perplexity(sentence, model, n=1):\n",
    "    \n",
    "    # Imports function to generate ngrams\n",
    "    from nltk.util import ngrams\n",
    "    \n",
    "    # List to store ngrams probabilities from sentence\n",
    "    sentence_ngrams = []\n",
    "    \n",
    "    # Iterates over ngrams in sentence\n",
    "    for n_gram in ngrams(sentence, n):\n",
    "        \n",
    "        # If unigrams\n",
    "        if n == 1:\n",
    "            \n",
    "            # Appends ngram probability\n",
    "            sentence_ngrams.append(model[n_gram[0]])\n",
    "        \n",
    "        # If not unigrams\n",
    "        else:\n",
    "            \n",
    "            # Appends ngram probability\n",
    "            sentence_ngrams.append(model[n_gram])\n",
    "    \n",
    "    # Calculates the number of ngrams\n",
    "    N = len(sentence_ngrams)\n",
    "    \n",
    "    # Returns zero if no ngrams are found\n",
    "    if N == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Imports methods to calculate log2 and power\n",
    "    from numpy import log2, power\n",
    "    \n",
    "    # Calculates log2 for each ngram probability\n",
    "    l = [ log2(item) for item in sentence_ngrams ]\n",
    "    \n",
    "    # Sums and averages the log2-probabilities\n",
    "    l = sum(l) / N\n",
    "    \n",
    "    # Returns the perplexity calculation\n",
    "    return power(2, -l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1289.55\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for news unigram model\n",
    "perplexity_sum = 0\n",
    "for sentence in news_test_data:\n",
    "    perplexity_sum += perplexity(sentence, news_unigram_model, n=1)  \n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(news_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 16230.65\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for news bigram model\n",
    "perplexity_sum = 0\n",
    "for sentence in news_test_data:\n",
    "    perplexity_sum += perplexity(sentence, news_bigram_model, n=2)\n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(news_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4588633.63\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for news trigram model\n",
    "perplexity_sum = 0\n",
    "for sentence in news_test_data:\n",
    "    perplexity_sum += perplexity(sentence, news_trigram_model, n=3)   \n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(news_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 806.15\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for blogs unigram model\n",
    "perplexity_sum = 0\n",
    "for sentence in blogs_test_data:\n",
    "    perplexity_sum += perplexity(sentence, blogs_unigram_model, n=1)\n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(blogs_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 18975.60\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for blogs bigram model\n",
    "perplexity_sum = 0\n",
    "for sentence in blogs_test_data:\n",
    "    perplexity_sum += perplexity(sentence, blogs_bigram_model, n=2)\n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(blogs_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4846085.30\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for blogs trigram model\n",
    "perplexity_sum = 0\n",
    "for sentence in blogs_test_data:\n",
    "    perplexity_sum += perplexity(sentence, blogs_trigram_model, n=3)\n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(blogs_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'i', 'am', 'consid', 'the', 'mx', 'num', 'probe', 'accord', 'corolla', 'and', 'the', 'numsx', '</s>']\n",
      "Unigram perplexity: 712.25\n",
      "Bigram perplexity: 2172.43\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'from', 'gsnum', 'prism', 'gatech', 'edu', 'glenn', 'r', 'stone', 'subject', 're', 'atf', 'burn', 'dividian', 'ranch', '</s>']\n",
      "Unigram perplexity: 2828.66\n",
      "Bigram perplexity: 1044.94\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'in', 'other', 'words', 'thei', 'let', 'the', 'best', 'honda', 'play', 'but', 'not', 'the', 'best', 'saturn', '</s>']\n",
      "Unigram perplexity: 642.48\n",
      "Bigram perplexity: 1565.45\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'he', 'wa', 'on', 'of', 'the', 'lead', 'scorer', 'on', 'a', 'mediocr', 'team', 'when', 'he', 'wa', 'trade', 'awai', 'in', 'num', '</s>']\n",
      "Unigram perplexity: 458.12\n",
      "Bigram perplexity: 761.32\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'pagliarulo', 'mike', 'num', 'num', 'num', 'num', 'num', 'thi', 'is', 'an', 'interest', 'line', '</s>']\n",
      "Unigram perplexity: 234.76\n",
      "Bigram perplexity: 287.48\n",
      "Trigram perplexity: 1136117.36\n",
      "['<s>', 'thank', 'for', 'your', 'help', '</s>']\n",
      "Unigram perplexity: 230.93\n",
      "Bigram perplexity: 278.78\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'other', 'bosnian', 'muslim', 'risk', 'their', 'life', 'to', 'hide', 'jew', 'from', 'the', 'nazi', 'and', 'ustashe', 'and', 'those', 'jew', 'who', 'surviv', 'the', 'war', 'rememb', 'that', '</s>']\n",
      "Unigram perplexity: 1176.20\n",
      "Bigram perplexity: 3248.80\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'but', 'then', 'what', 'would', 'you', 'expect', 'from', 'a', 'bunch', 'of', 'peopl', 'who', 'can', 't', 'even', 'agre', 'on', 'the', 'phase', 'of', 'the', 'moon', '</s>']\n",
      "Unigram perplexity: 384.91\n",
      "Bigram perplexity: 444.19\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'let', 'me', 'present', 'the', 'barest', 'outlin', 'of', 'a', 'differ', 'view', 'of', 'text', 'and', 'their', 'us', 'in', 'studi', 'history', '</s>']\n",
      "Unigram perplexity: 816.06\n",
      "Bigram perplexity: 3580.27\n",
      "Trigram perplexity: 4828531.00\n",
      "['<s>', 'frank', 'crari', 'cu', 'boulder', 'i', 'agree', '</s>']\n",
      "Unigram perplexity: 2834.73\n",
      "Bigram perplexity: 3285.03\n",
      "Trigram perplexity: 4828531.00\n"
     ]
    }
   ],
   "source": [
    "# Prints perplexity for ten sentences and three models\n",
    "for i in range(10):\n",
    "    print(news_test_data[i])\n",
    "    print(\"Unigram perplexity: {:6.2f}\".format(perplexity(news_test_data[i], news_unigram_model, n=1)))\n",
    "    print(\"Bigram perplexity: {:6.2f}\".format(perplexity(news_test_data[i], news_bigram_model, n=2)))\n",
    "    print(\"Trigram perplexity: {:6.2f}\".format(perplexity(news_test_data[i], news_trigram_model, n=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a linear interpolation model\n",
    "def linear_ngram(l_a, l_b, unigram, bigram):\n",
    "    linear = {}\n",
    "    for w1, w2 in bigram.keys():\n",
    "        linear[(w1, w2)] = l_a * bigram[(w1, w2)] + l_b * unigram[w1]\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the best model using SGD \n",
    "def sgd_linear_ngram(validation, unigram, bigram, n=30, alpha=1e-3):\n",
    "    \n",
    "    lambdas = [0.1, 0.9]\n",
    "    \n",
    "    P_prev = 0\n",
    "    \n",
    "    for indx in range(n):\n",
    "        \n",
    "        linear = linear_ngram(lambdas[0], lambdas[1], unigram, bigram)\n",
    "        \n",
    "        p_sum = 0\n",
    "        \n",
    "        for sentence in validation:\n",
    "            \n",
    "            p_sum += perplexity(sentence, linear, n=2)\n",
    "            \n",
    "        P = p_sum / len(validation)\n",
    "            \n",
    "        dP = P - P_prev\n",
    "        P_prev = P\n",
    "        \n",
    "        lambdas[0] -= alpha * dP\n",
    "        lambdas[1] -= alpha * dP\n",
    "        \n",
    "        sum_l = sum(lambdas)\n",
    "        \n",
    "        lambdas[0] /= sum_l\n",
    "        lambdas[1] /= sum_l\n",
    "    \n",
    "    return linear, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculates the best linear model for news dataset\n",
    "news_linear, news_lambdas = sgd_linear_ngram(news_test_data, news_unigram_model, news_bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the best linear model for blogs dataset\n",
    "blogs_linear, blogs_lambdas = sgd_linear_ngram(blogs_test_data, blogs_unigram_model, blogs_bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43810584607794384, 0.5618941539220561]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39353838332215585, 0.6064616166778443]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1258.02\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for news linear model\n",
    "perplexity_sum = 0\n",
    "for sentence in news_test_data:\n",
    "    perplexity_sum += perplexity(sentence, news_linear, n=2)\n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(news_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 967.98\n"
     ]
    }
   ],
   "source": [
    "# Calculates perplexity for blogs linear model\n",
    "perplexity_sum = 0\n",
    "for sentence in blogs_test_data:\n",
    "    perplexity_sum += perplexity(sentence, blogs_linear, n=2)\n",
    "print(\"Perplexity: {:4.2f}\".format(perplexity_sum / len(blogs_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
