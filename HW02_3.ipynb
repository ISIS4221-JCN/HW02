{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B-_Ey_ZnIdkg"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cMhJWQ5JIgBU"
   },
   "outputs": [],
   "source": [
    "files = ['positive', 'negative']\n",
    "categories = ['books', 'dvd', 'kitchen', 'electronics']\n",
    "characters_to_remove = '!()#@~\"'\n",
    "pattern = \"[\" + characters_to_remove + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_score = {}\n",
    "pos_score = {}\n",
    "with open('./data/EN_Lexicons/SentiWordNet_3.0.0.txt') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                splitted = line.split('\\t')[:5]\n",
    "                pos = {splitted[-1].split('#')[0]: float(splitted[2])}\n",
    "                neg = {splitted[-1].split('#')[0]: float(splitted[3])}\n",
    "                neg_score.update(neg)\n",
    "                pos_score.update(pos)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANFII = {}\n",
    "with open('./data/EN_Lexicons/AFINN-111.txt') as file:\n",
    "    for line in file:\n",
    "        item = line.strip('\\n').split('\\t')\n",
    "        d = {item[0]: int(item[1])}\n",
    "        ANFII.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stat_pos = []\n",
    "word_stat_neg = []\n",
    "neg_words = False\n",
    "pos_words = False\n",
    "with open('./data/EN_Lexicons/WordStat Sentiments.txt') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if line.strip('\\t').replace(' ', '_').replace('\\n', '') == 'NEGATIVE_WORDS':\n",
    "            neg_words = True\n",
    "        if neg_words:\n",
    "            word_stat_neg.append(line.strip('\\t').split(' ')[0].replace(' ','_').lower())\n",
    "        if line.strip('\\t').replace(' ', '_').replace('\\n', '') == 'POSITIVE_WORDS':\n",
    "            neg_words = False\n",
    "            pos_words = True\n",
    "        if pos_words:\n",
    "            word_stat_pos.append(line.strip('\\t').split(' ')[0].replace(' ','_').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./data/EN_Lexicons/senticnet5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ZfCuTy1VH_9n"
   },
   "outputs": [],
   "source": [
    "def read_file(category, file):\n",
    "    words = []\n",
    "    docs = []\n",
    "    tags = []\n",
    "    with open('./data/SA/'+category+'/'+file+'.review', encoding='ISO-8859-1') as file:\n",
    "        for line in file:\n",
    "            words_and_freq = re.sub(pattern, \"\", line.strip('\\n').strip('\\x1a')).split('label:')\n",
    "            if words_and_freq[-1] == 'positive':\n",
    "                tag = int(1)\n",
    "            else:\n",
    "                tag = int(0)\n",
    "            tags.append(tag)\n",
    "            words_and_freq = words_and_freq[0].split(' ')\n",
    "            d = {}\n",
    "            for term in words_and_freq[:-1]:\n",
    "                split = term.split(':')\n",
    "                words.append(split[0])\n",
    "                x = {split[0]:int(split[1])}\n",
    "                d.update(x)\n",
    "            docs.append(d)\n",
    "    return [words, docs, tags]\n",
    "\n",
    "def build_dataset(category):\n",
    "    dictionary = []\n",
    "    documents = []\n",
    "    tags = []\n",
    "    for file in files:\n",
    "        [words_temp, docs_temp, tags_temp] = read_file(category, file)\n",
    "        words_temp = np.unique(words_temp)\n",
    "        for term in zip(docs_temp, tags_temp):\n",
    "            documents.append(term[0])\n",
    "            tags.append(term[1])\n",
    "        for word in words_temp:\n",
    "            dictionary.append(word)\n",
    "    temp, unlabeled_docs, unlabeled_tags = read_file(category, 'unlabeled')\n",
    "    return [np.array(np.unique(dictionary),dtype='str'), documents, tags, unlabeled_docs, unlabeled_tags]\n",
    "  \n",
    "def purge_dataset(dictionary, documents):\n",
    "    freq = np.zeros(len(dictionary))\n",
    "    for i, word in enumerate(dictionary):\n",
    "        for doc in documents:\n",
    "            try:\n",
    "                freq[i] = freq[i] + doc[word]\n",
    "            except:\n",
    "                pass\n",
    "    to_remove = []\n",
    "    for i, occurence in enumerate(zip(dictionary, freq)):\n",
    "        if occurence[1] == 1:\n",
    "            to_remove.append(i)\n",
    "    dictionary = np.delete(dictionary, to_remove)\n",
    "    return dictionary\n",
    "\n",
    "def build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, file, boolean):\n",
    "    if file == 'labeled':\n",
    "        m = np.zeros((len(dictionary) + 1, len(documents)), dtype = np.bool_ if boolean == 'bool' else np.int8)\n",
    "        for i, document in enumerate(documents):\n",
    "            for key in document.keys():\n",
    "                index = np.where(dictionary == key)[0]\n",
    "                if len(index) != 0:\n",
    "                    m[index[0],i] = True if boolean == 'bool' else document[key]\n",
    "            m[-1, i] = tags[i]\n",
    "    else:\n",
    "        m = np.zeros((len(dictionary) + 1, len(unlabeled_docs)), dtype = np.bool_ if boolean == 'bool' else np.int8)\n",
    "        for i, document in enumerate(unlabeled_docs):\n",
    "            for key in document.keys():\n",
    "                index = np.where(dictionary == key)[0]\n",
    "                if len(index) != 0:\n",
    "                    m[index[0],i] = True if boolean == 'bool' else document[key]\n",
    "            m[-1, i] = unlabeled_tags[i]\n",
    "    return m\n",
    "\n",
    "def set_values(pos_count, neg_count, polarity_last, pos_sum, neg_sum, ANFII_pos_count, ANFII_neg_count, \n",
    "                      ANFII_pos_score, ANFII_neg_score, stat_pos, stat_neg, polarity_senticnet, file):\n",
    "    if neg_count == 0:\n",
    "        div = 0\n",
    "    else:\n",
    "        div = pos_count/neg_count\n",
    "    if file == 'positive':\n",
    "        tag = 1\n",
    "    else:\n",
    "        tag = 0\n",
    "    return np.array([pos_count, neg_count, polarity_last, pos_sum, neg_sum, ANFII_pos_count, ANFII_neg_count, \n",
    "                      ANFII_pos_score, ANFII_neg_score, stat_pos, stat_neg, polarity_senticnet, tag])\n",
    "\n",
    "def create_lexicon_vector(doc, file):\n",
    "    neg_count = pos_count = neg_sum = pos_sum = 0\n",
    "    ANFII_pos_count = ANFII_neg_count = ANFII_pos_score = ANFII_neg_score = 0\n",
    "    stat_pos = stat_neg = polarity_senticnet = 0\n",
    "    last_word = True; word_in_lex = False\n",
    "    words = list(doc.keys())\n",
    "    polarity_last = 0\n",
    "    for word in reversed(words):\n",
    "        list_words = word.split('_')\n",
    "        for list_word in list_words:\n",
    "            if list_word in neg_score.keys():\n",
    "                if neg_score[list_word] > 0:\n",
    "                    neg_count += 1\n",
    "                    neg_sum += neg_score[list_word]\n",
    "                    word_in_lex = True\n",
    "                if pos_score[list_word] > 0:\n",
    "                    pos_count += 1\n",
    "                    pos_sum += pos_score[list_word]\n",
    "                    word_in_lex = True\n",
    "                if word_in_lex and last_word:\n",
    "                    polarity_last = (neg_score[list_word]*-1) + (pos_score[list_word])\n",
    "                    last_word = False\n",
    "            if list_word in ANFII.keys():\n",
    "                if ANFII[list_word] > 0:\n",
    "                    ANFII_pos_count += 1\n",
    "                    ANFII_pos_score += ANFII[list_word]\n",
    "                else:\n",
    "                    ANFII_neg_count += 1\n",
    "                    ANFII_neg_score += ANFII[list_word]\n",
    "            if list_word in word_stat_pos:\n",
    "                stat_pos += 1\n",
    "            elif list_word in word_stat_neg:\n",
    "                stat_neg += 1\n",
    "            if list_word in list(senticnet.keys()):\n",
    "                polarity_senticnet += float(senticnet[list_word][7])\n",
    "    return set_values(pos_count, neg_count, polarity_last, pos_sum, neg_sum, ANFII_pos_count, ANFII_neg_count, \n",
    "                      ANFII_pos_score, ANFII_neg_score, stat_pos, stat_neg, polarity_senticnet, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWl2sJGFhV1s",
    "outputId": "87ad74cd-2970-4642-b45d-04786118ed8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [54:30<00:00, 817.72s/it]   \n"
     ]
    }
   ],
   "source": [
    "for cat in tqdm(categories):\n",
    "    [dictionary, documents, tags, unlabeled_docs, unlabeled_tags] = build_dataset(cat)\n",
    "    dictionary = purge_dataset(dictionary, documents)\n",
    "    np.save('./data/SA/'+cat+'/dictionary.npy', dictionary)\n",
    "    ans = build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'bool')\n",
    "    np.save('./data/SA/'+cat+'/bool-labeled.npy',ans)\n",
    "    ans = build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'not-bool')\n",
    "    np.save('./data/SA/'+cat+'/unlabeled.npy',ans)\n",
    "    ans = build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'not-bool')\n",
    "    np.save('./data/SA/'+cat+'/labeled.npy',ans)\n",
    "    ans = build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'bool')\n",
    "    np.save('./data/SA/'+cat+'/bool-unlabeled.npy',ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zX6jWMTPomF6",
    "outputId": "fb3f1a2a-313a-4d50-fb65-0cbb87661383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100801,)\n"
     ]
    }
   ],
   "source": [
    "dictionary = np.concatenate((np.load('./data/SA/books/dictionary.npy'),\n",
    "                            np.load('./data/SA/dvd/dictionary.npy'),\n",
    "                            np.load('./data/SA/electronics/dictionary.npy'),\n",
    "                            np.load('./data/SA/kitchen/dictionary.npy')), axis = 0)\n",
    "dictionary = np.array(np.unique(dictionary),dtype='str')\n",
    "print(dictionary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "0eYYrMIS-eKW",
    "outputId": "87c241c4-4f9e-4f7a-fae0-c453e0a90b1d"
   },
   "outputs": [],
   "source": [
    "[dictionary1, documents, tags, unlabeled_docs, unlabeled_tags] = build_dataset('books')\n",
    "np.save('./data/SA/all/bool-labeled.npy', build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'bool'))\n",
    "np.save('./data/SA/all/bool-unlabeled.npy', build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'bool'))\n",
    "np.save('./data/SA/all/labeled.npy', build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'not-bool'))\n",
    "np.save('./data/SA/all/unlabeled.npy', build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'not-bool'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ev0gH3baDEOw"
   },
   "outputs": [],
   "source": [
    "[dictionary1, documents, tags, unlabeled_docs, unlabeled_tags] = build_dataset('dvd')\n",
    "m = np.load('./data/SA/all/bool-labeled.npy')\n",
    "np.save('./data/SA/all/bool-labeled.npy', \n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/bool-unlabeled.npy')\n",
    "np.save('./data/SA/all/bool-unlabeled.npy',\n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/labeled.npy')\n",
    "np.save('./data/SA/all/labeled.npy',\n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'not-bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/unlabeled.npy')\n",
    "np.save('./data/SA/all/unlabeled.npy', \n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'not-bool')), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OYMv45NWEPDz"
   },
   "outputs": [],
   "source": [
    "[dictionary1, documents, tags, unlabeled_docs, unlabeled_tags] = build_dataset('electronics')\n",
    "m = np.load('./data/SA/all/bool-labeled.npy')\n",
    "np.save('./data/SA/all/bool-labeled.npy', \n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/bool-unlabeled.npy')\n",
    "np.save('./data/SA/all/bool-unlabeled.npy',\n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/labeled.npy')\n",
    "np.save('./data/SA/all/labeled.npy',\n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'not-bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/unlabeled.npy')\n",
    "np.save('./data/SA/all/unlabeled.npy', \n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'not-bool')), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KreTuVuXEQkj"
   },
   "outputs": [],
   "source": [
    "[dictionary1, documents, tags, unlabeled_docs, unlabeled_tags] = build_dataset('kitchen')\n",
    "m = np.load('./data/SA/all/bool-labeled.npy')\n",
    "np.save('./data/SA/all/bool-labeled.npy', \n",
    "       np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/bool-unlabeled.npy')\n",
    "np.save('./data/SA/all/bool-unlabeled.npy',\n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/labeled.npy')\n",
    "np.save('./data/SA/all/labeled.npy',\n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'labeled', 'not-bool')), axis = 1))\n",
    "m = np.load('./data/SA/all/unlabeled.npy')\n",
    "np.save('./data/SA/all/unlabeled.npy', \n",
    "        np.concatenate((m, build_matrix(dictionary, documents, tags, unlabeled_docs, unlabeled_tags, 'unlabeled', 'not-bool')), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ojL6RdHbFRWa"
   },
   "outputs": [],
   "source": [
    "np.save('./data/SA/all/dictionary.npy', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [10:50:08<00:00, 9752.08s/it]  \n"
     ]
    }
   ],
   "source": [
    "for cat in tqdm(['books', 'dvd', 'electronics', 'kitchen']):\n",
    "    m = np.zeros((1,13))\n",
    "    for file in ['positive', 'negative']:\n",
    "        words, docs, tags = read_file('books', file)\n",
    "        for doc in docs:\n",
    "            n = np.reshape(create_lexicon_vector(doc, file), (1,13))\n",
    "            m = np.append(m, n, axis=0)\n",
    "    m[~np.all(m == 0, axis=1)]\n",
    "    np.save('./data/SA/'+cat+'/lexicon-labeled.npy', np.transpose(m[1:,:]))\n",
    "    words, docs, tags = read_file('books', 'unlabeled')\n",
    "    m = np.zeros((1,13))\n",
    "    for doc in docs:\n",
    "        n = np.reshape(create_lexicon_vector(doc, file), (1,13))\n",
    "        m = np.append(m, n, axis=0)\n",
    "    m[~np.all(m == 0, axis=1)]\n",
    "    np.save('./data/SA/'+cat+'/lexicon-unlabeled.npy', np.transpose(m[1:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/SA/all/lexicon-labeled.npy', np.concatenate((np.load('./data/SA/books/lexicon-labeled.npy'),\n",
    "                                                             np.load('./data/SA/dvd/lexicon-labeled.npy'),\n",
    "                                                             np.load('./data/SA/electronics/lexicon-labeled.npy'),\n",
    "                                                             np.load('./data/SA/kitchen/lexicon-labeled.npy')), \n",
    "                                                             axis=1))\n",
    "\n",
    "np.save('./data/SA/all/lexicon-unlabeled.npy', np.concatenate((np.load('./data/SA/books/lexicon-unlabeled.npy'),\n",
    "                                                             np.load('./data/SA/dvd/lexicon-unlabeled.npy'),\n",
    "                                                             np.load('./data/SA/electronics/lexicon-unlabeled.npy'),\n",
    "                                                             np.load('./data/SA/kitchen/lexicon-unlabeled.npy')), \n",
    "                                                             axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW02_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
