{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secure-passenger",
   "metadata": {},
   "source": [
    "# 20N Newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attempted-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "import os, nltk, re\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from smart_open import smart_open\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "competitive-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_remove = '!()#@~,.\"><*=-'\n",
    "pattern = \"[\" + characters_to_remove + \"]\"\n",
    "p = PorterStemmer()\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "freq_to_remove = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extended-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(p, tokenizer, text):\n",
    "    \"\"\" Applies standard pre-processing to given text.\n",
    "    \n",
    "    Args:\n",
    "        p (gensim.parsing.porter.PorterStemmer): stemmer object.\n",
    "        tokenizer (nltk.tokenize.regexp.RegexpTokenizer): tokenizr object.\n",
    "        text (str): text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "        list: preprocessed text.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Converts to lowercase\n",
    "    doc_nor = text.lower()\n",
    "    \n",
    "    # Removes stopwords\n",
    "    doc_sw = remove_stopwords(doc_nor)\n",
    "    \n",
    "    # Stems text\n",
    "    doc_stem = p.stem_sentence(doc_sw)\n",
    "    \n",
    "    # Lemmatizes text\n",
    "    # TODO: Lemmatizer\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return tokenizer.tokenize(doc_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mysterious-sixth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comp.sys.ibm.pc.hardware': 0, 'soc.religion.christian': 1, 'sci.med': 2, 'talk.politics.misc': 3, 'talk.religion.misc': 4, 'comp.os.ms-windows.misc': 5, 'sci.crypt': 6, 'alt.atheism': 7, 'sci.space': 8, 'talk.politics.guns': 9, 'talk.politics.mideast': 10, 'comp.graphics': 11, 'rec.motorcycles': 12, 'comp.windows.x': 13, 'comp.sys.mac.hardware': 14, 'rec.autos': 15, 'rec.sport.hockey': 16, 'rec.sport.baseball': 17, 'sci.electronics': 18, 'misc.forsale': 19}\n"
     ]
    }
   ],
   "source": [
    "categories = os.listdir('./data/20news')\n",
    "category_index = {}\n",
    "for i, cat in enumerate(categories):\n",
    "    d = {cat: i}\n",
    "    category_index.update(d)\n",
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "animated-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_text = []\n",
    "listed_categories = []\n",
    "for category in categories:\n",
    "    files = os.listdir('./data/20news/' + category)\n",
    "    for file in files:\n",
    "        doc = open('./data/20news/' + category + '/' + file, encoding = 'ISO-8859-1',mode='r')\n",
    "        text = re.sub(pattern, \"\", doc.read().replace('\\n', '').replace('  ', ''))\n",
    "        listed_text.append(process(p, tokenizer, text))\n",
    "        listed_categories.append(category)\n",
    "        doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indonesian-diploma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 100000\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(listed_text)\n",
    "dictionary.filter_extremes(no_below=freq_to_remove)\n",
    "dictionary.save('./resources/20news/vocab20news.dict')\n",
    "doc_corpus = []\n",
    "for doc in listed_text:\n",
    "    doc_corpus.append(dictionary.doc2bow(doc))\n",
    "print('Dictionary length: ' + str(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acknowledged-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_bow = np.zeros((len(doc_corpus), len(dictionary) + 1), dtype=np.int8)\n",
    "bow = np.zeros((len(doc_corpus), len(dictionary) + 1), dtype=np.int8)\n",
    "for index, doc in enumerate(doc_corpus):\n",
    "    bool_bow[index, -1] = category_index[listed_categories[index]]\n",
    "    bow[index, -1] = category_index[listed_categories[index]]\n",
    "    for item in doc:\n",
    "        bool_bow[index, item[0]] = 1\n",
    "        bow[index, item[0]] = item[1]\n",
    "np.save('./resources/20news/bool_bow_matrix.npy', bool_bow)\n",
    "np.save('./resources/20news/bow_matrix.npy', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atomic-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X, y,train_size=0.6, val_size=0.1, test_size=0.3):\n",
    "    if not(train_size + val_size + test_size == 1):\n",
    "        raise Exception('Sizes must add up to exactely 1.0')\n",
    "    X_train, X_val, y_train,y_val = train_test_split(X, y, train_size = train_size, random_state = 15)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, train_size = val_size/(val_size+test_size),\n",
    "                                                                                        random_state=15)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def train_validate_evaluate(classifier, X_train, X_val,y_train, y_val):\n",
    "    \"\"\" Trains and evaluates specified classifier\n",
    "    \n",
    "    Args:\n",
    "        classifier (str): initials of classifier\n",
    "        dataset (str): category to be trained on\n",
    "        feature (str): file to use as training data\n",
    "        \n",
    "    Returns:\n",
    "        'pandas.dataframe': Dataframe containing metrics for each classifier    \n",
    "    \"\"\"\n",
    "    if classifier == 'NB':\n",
    "        clf = GaussianNB()\n",
    "    elif classifier == 'LR':\n",
    "        clf = LogisticRegression(random_state=0, max_iter=500)\n",
    "    clf.fit(X_train, y_train)\n",
    "    if classifier == 'LR':\n",
    "        LR_coeffs.append(clf.coef_)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "    precision = metrics.precision_score(y_val, y_pred)\n",
    "    recall = metrics.recall_score(y_val, y_pred)\n",
    "    f1_score = metrics.f1_score(y_val, y_pred)\n",
    "    metrics_data = {'dataset': [dataset], 'classifier': [classifier], 'model':[feature],\n",
    "        'accuracy': [accuracy], 'precision':[precision], 'recall':[recall], 'f1_score': [f1_score]}\n",
    "    df = pd.DataFrame(data = metrics_data)\n",
    "    df.index = [dataset + ' ' + classifier + ' ' + feature]\n",
    "    return clf, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-celtic",
   "metadata": {},
   "source": [
    "# Pending train for each classifier and custom feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sophisticated-brick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 9)\n",
      "(10, 9)\n",
      "(30, 9)\n",
      "(60,)\n",
      "(10,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "metrics = pd.DataFrame()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = data_split(bow[:,:-1], bow[:,-1])\n",
    "clf, df = train_validate_evaluaite('NB', X_train, X_val, y_train, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
