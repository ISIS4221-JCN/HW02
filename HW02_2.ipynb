{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secure-passenger",
   "metadata": {},
   "source": [
    "# 20N Newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attempted-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "import os, nltk, re\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from smart_open import smart_open\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "competitive-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_remove = '!()#@~,.\"><*=-'\n",
    "pattern = \"[\" + characters_to_remove + \"]\"\n",
    "p = PorterStemmer()\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "freq_to_remove = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extended-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(p, tokenizer, text):\n",
    "    \"\"\" Applies standard pre-processing to given text.\n",
    "    \n",
    "    Args:\n",
    "        p (gensim.parsing.porter.PorterStemmer): stemmer object.\n",
    "        tokenizer (nltk.tokenize.regexp.RegexpTokenizer): tokenizr object.\n",
    "        text (str): text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "        list: preprocessed text.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Converts to lowercase\n",
    "    doc_nor = text.lower()\n",
    "    \n",
    "    # Removes stopwords\n",
    "    doc_sw = remove_stopwords(doc_nor)\n",
    "    \n",
    "    # Stems text\n",
    "    doc_stem = p.stem_sentence(doc_sw)\n",
    "    \n",
    "    # Lemmatizes text\n",
    "    # TODO: Lemmatizer\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return tokenizer.tokenize(doc_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mysterious-sixth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comp.sys.ibm.pc.hardware': 0, 'soc.religion.christian': 1, 'sci.med': 2, 'talk.politics.misc': 3, 'talk.religion.misc': 4, 'comp.os.ms-windows.misc': 5, 'sci.crypt': 6, 'alt.atheism': 7, 'sci.space': 8, 'talk.politics.guns': 9, 'talk.politics.mideast': 10, 'comp.graphics': 11, 'rec.motorcycles': 12, 'comp.windows.x': 13, 'comp.sys.mac.hardware': 14, 'rec.autos': 15, 'rec.sport.hockey': 16, 'rec.sport.baseball': 17, 'sci.electronics': 18, 'misc.forsale': 19}\n"
     ]
    }
   ],
   "source": [
    "categories = os.listdir('./data/20news')\n",
    "category_index = {}\n",
    "for i, cat in enumerate(categories):\n",
    "    d = {cat: i}\n",
    "    category_index.update(d)\n",
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "animated-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_text = []\n",
    "listed_categories = []\n",
    "for category in categories:\n",
    "    files = os.listdir('./data/20news/' + category)\n",
    "    for file in files:\n",
    "        doc = open('./data/20news/' + category + '/' + file, encoding = 'ISO-8859-1',mode='r')\n",
    "        text = re.sub(pattern, \"\", doc.read().replace('\\n', '').replace('  ', ''))\n",
    "        listed_text.append(process(p, tokenizer, text))\n",
    "        listed_categories.append(category)\n",
    "        doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indonesian-diploma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 100000\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(listed_text)\n",
    "dictionary.filter_extremes(no_below=freq_to_remove)\n",
    "dictionary.save('./resources/20news/vocab20news.dict')\n",
    "doc_corpus = []\n",
    "for doc in listed_text:\n",
    "    doc_corpus.append(dictionary.doc2bow(doc))\n",
    "print('Dictionary length: ' + str(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acknowledged-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_bow = np.zeros((len(doc_corpus), len(dictionary) + 1), dtype=np.int8)\n",
    "bow = np.zeros((len(doc_corpus), len(dictionary) + 1), dtype=np.int8)\n",
    "for index, doc in enumerate(doc_corpus):\n",
    "    bool_bow[index, -1] = category_index[listed_categories[index]]\n",
    "    bow[index, -1] = category_index[listed_categories[index]]\n",
    "    for item in doc:\n",
    "        bool_bow[index, item[0]] = 1\n",
    "        bow[index, item[0]] = item[1]\n",
    "np.save('./resources/20news/bool_bow_matrix.npy', bool_bow)\n",
    "np.save('./resources/20news/bow_matrix.npy', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-switzerland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
