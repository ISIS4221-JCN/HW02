{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "HW02_2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "secure-passenger"
      },
      "source": [
        "# 20N Newsgroup"
      ],
      "id": "secure-passenger"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYQl4foTjQEi",
        "outputId": "0c222ca9-b096-42e7-f709-3c09d6e3bb82"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BYQl4foTjQEi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attempted-outline"
      },
      "source": [
        "from xml.dom import minidom\n",
        "import os, nltk, re\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "from gensim import similarities\n",
        "from smart_open import smart_open\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "attempted-outline",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "competitive-history"
      },
      "source": [
        "characters_to_remove = '!()#@~,.\"><*=-'\n",
        "pattern = \"[\" + characters_to_remove + \"]\"\n",
        "p = PorterStemmer()\n",
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
        "freq_to_remove = 1"
      ],
      "id": "competitive-history",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extended-genome"
      },
      "source": [
        "def process(p, tokenizer, text):\n",
        "    \"\"\" Applies standard pre-processing to given text.\n",
        "    \n",
        "    Args:\n",
        "        p (gensim.parsing.porter.PorterStemmer): stemmer object.\n",
        "        tokenizer (nltk.tokenize.regexp.RegexpTokenizer): tokenizr object.\n",
        "        text (str): text to preprocess.\n",
        "    \n",
        "    Returns:\n",
        "        list: preprocessed text.\n",
        "    \n",
        "    \"\"\"\n",
        "    # Converts to lowercase\n",
        "    doc_nor = text.lower()\n",
        "    \n",
        "    # Removes stopwords\n",
        "    doc_sw = remove_stopwords(doc_nor)\n",
        "    \n",
        "    # Stems text\n",
        "    doc_stem = p.stem_sentence(doc_sw)\n",
        "    \n",
        "    # Lemmatizes text\n",
        "    # TODO: Lemmatizer\n",
        "    \n",
        "    # Returns preprocessed text\n",
        "    return tokenizer.tokenize(doc_stem)"
      ],
      "id": "extended-genome",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mysterious-sixth",
        "outputId": "9b42ac67-2341-4af9-a1a4-4a832aef769a"
      },
      "source": [
        "categories = os.listdir('/content/drive/MyDrive/data/20news')\n",
        "category_index = {}\n",
        "for i, cat in enumerate(categories):\n",
        "    d = {cat: i}\n",
        "    category_index.update(d)\n",
        "print(category_index)"
      ],
      "id": "mysterious-sixth",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'talk.politics.mideast': 0, 'rec.motorcycles': 1, 'rec.autos': 2, 'comp.windows.x': 3, 'comp.sys.mac.hardware': 4, 'misc.forsale': 5, 'comp.graphics': 6, 'rec.sport.baseball': 7, 'rec.sport.hockey': 8, 'sci.electronics': 9, 'talk.politics.guns': 10, 'talk.religion.misc': 11, 'comp.os.ms-windows.misc': 12, 'sci.space': 13, 'sci.crypt': 14, 'comp.sys.ibm.pc.hardware': 15, 'soc.religion.christian': 16, 'talk.politics.misc': 17, 'sci.med': 18, 'alt.atheism': 19}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "animated-saver"
      },
      "source": [
        "listed_text = []\n",
        "listed_categories = []\n",
        "for category in categories:\n",
        "    files = os.listdir('/content/drive/MyDrive/data/20news/' + category)\n",
        "    for file in files:\n",
        "        doc = open('/content/drive/MyDrive/data/20news/' + category + '/' + file, encoding = 'ISO-8859-1',mode='r')\n",
        "        text = re.sub(pattern, \"\", doc.read().replace('\\n', '').replace('  ', ''))\n",
        "        listed_text.append(process(p, tokenizer, text))\n",
        "        listed_categories.append(category)\n",
        "        doc.close()"
      ],
      "id": "animated-saver",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "indonesian-diploma"
      },
      "source": [
        "dictionary = corpora.Dictionary(listed_text)\n",
        "dictionary.filter_extremes(no_below=freq_to_remove)\n",
        "dictionary.save('/content/drive/MyDrive/resources/20news/vocab20news.dict')\n",
        "doc_corpus = []\n",
        "for doc in listed_text:\n",
        "    doc_corpus.append(dictionary.doc2bow(doc))\n",
        "print('Dictionary length: ' + str(len(dictionary)))"
      ],
      "id": "indonesian-diploma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acknowledged-connecticut"
      },
      "source": [
        "bool_bow = np.zeros((len(doc_corpus), len(dictionary) + 1), dtype=np.int8)\n",
        "bow = np.zeros((len(doc_corpus), len(dictionary) + 1), dtype=np.int8)\n",
        "for index, doc in enumerate(doc_corpus):\n",
        "    bool_bow[index, -1] = category_index[listed_categories[index]]\n",
        "    bow[index, -1] = category_index[listed_categories[index]]\n",
        "    for item in doc:\n",
        "        bool_bow[index, item[0]] = 1\n",
        "        bow[index, item[0]] = item[1]\n",
        "np.save('/content/drive/MyDrive/resources/20news/bool_bow_matrix.npy', bool_bow)\n",
        "np.save('/content/drive/MyDrive/resources/20news/bow_matrix.npy', bow)"
      ],
      "id": "acknowledged-connecticut",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atomic-switzerland"
      },
      "source": [
        "def data_split(X, y,train_size=0.6, val_size=0.1, test_size=0.3):\n",
        "    if not(train_size + val_size + test_size == 1):\n",
        "        raise Exception('Sizes must add up to exactely 1.0')\n",
        "    X_train, X_val, y_train,y_val = train_test_split(X, y, train_size = train_size, random_state = 15)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, train_size = val_size/(val_size+test_size),\n",
        "                                                                                        random_state=15)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "def train_validate_evaluate(classifier, X_train, X_val,y_train, y_val, feature):\n",
        "    \"\"\" Trains and evaluates specified classifier\n",
        "    \n",
        "    Args:\n",
        "        classifier (str): initials of classifier\n",
        "        dataset (str): category to be trained on\n",
        "        feature (str): file to use as training data\n",
        "        \n",
        "    Returns:\n",
        "        'pandas.dataframe': Dataframe containing metrics for each classifier    \n",
        "    \"\"\"\n",
        "    if classifier == 'NB':\n",
        "        clf = GaussianNB()\n",
        "    elif classifier == 'LR':\n",
        "        clf = LogisticRegression(random_state=0, max_iter=500)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    accuracy = metrics.accuracy_score(y_val, y_pred)\n",
        "    precision = metrics.precision_score(y_val, y_pred, average='macro')\n",
        "    recall = metrics.recall_score(y_val, y_pred, average = 'macro')\n",
        "    f1_score = metrics.f1_score(y_val, y_pred,average='macro')\n",
        "    metrics_data = {'classifier': [classifier], 'model':[feature],\n",
        "        'accuracy': [accuracy], 'precision':[precision], 'recall':[recall], 'f1_score': [f1_score]}\n",
        "    df = pd.DataFrame(data = metrics_data)\n",
        "    df.index = [classifier + ' ' + feature]\n",
        "    return clf, df"
      ],
      "id": "atomic-switzerland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "earlier-celtic"
      },
      "source": [
        "# Pending train for each classifier and custom feature extraction"
      ],
      "id": "earlier-celtic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sophisticated-brick"
      },
      "source": [
        "X_train = np.load('/content/drive/MyDrive/data/bow_matrix.npy')\n",
        "metrics_df = pd.DataFrame()\n",
        "clfs = []\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = data_split(X_train[:,:-1], X_train[:,-1])\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
        "precision = metrics.precision_score(y_val, y_pred, average='macro')\n",
        "recall = metrics.recall_score(y_val, y_pred, average = 'macro')\n",
        "f1_score = metrics.f1_score(y_val, y_pred,average='macro')\n",
        "metrics_data = {'classifier': ['NB'], 'model':['bow'],\n",
        "    'accuracy': [accuracy], 'precision':[precision], 'recall':[recall], 'f1_score': [f1_score]}\n",
        "df = pd.DataFrame(data = metrics_data)\n",
        "metrics_df = pd.concat([metrics_df, df], axis = 1)\n",
        "clfs.append(clf)"
      ],
      "id": "sophisticated-brick",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibp80QHi_Boh"
      },
      "source": [
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
        "precision = metrics.precision_score(y_val, y_pred, average='macro')\n",
        "\n",
        "recall = metrics.recall_score(y_val, y_pred, average = 'macro')\n",
        "f1_score = metrics.f1_score(y_val, y_pred,average='macro')\n",
        "metrics_data = {'classifier': ['NB'], 'model':['bow'],\n",
        "    'accuracy': [accuracy], 'precision':[precision], 'recall':[recall], 'f1_score': [f1_score]}\n",
        "df = pd.DataFrame(data = metrics_data)\n",
        "metrics_df = pd.concat([metrics_df, df], axis = 1)\n",
        "\n",
        "clfs.append(clf)"
      ],
      "id": "ibp80QHi_Boh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "instructional-chambers"
      },
      "source": [
        "X_train = np.load('/content/drive/MyDrive/data/bool_bow_matrix.npy')\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = data_split(X_train[:,:-1], X_train[:,-1])\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
        "precision = metrics.precision_score(y_val, y_pred, average='macro')\n",
        "recall = metrics.recall_score(y_val, y_pred, average = 'macro')\n",
        "f1_score = metrics.f1_score(y_val, y_pred,average='macro')\n",
        "metrics_data = {'classifier': ['NB'], 'model':['bool_bow'],\n",
        "    'accuracy': [accuracy], 'precision':[precision], 'recall':[recall], 'f1_score': [f1_score]}\n",
        "df = pd.DataFrame(data = metrics_data)\n",
        "metrics_df = pd.concat([metrics_df, df], axis = 1)\n",
        "clfs.append(clf)"
      ],
      "id": "instructional-chambers",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "competent-kentucky"
      },
      "source": [
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
        "precision = metrics.precision_score(y_val, y_pred, average='macro')\n",
        "recall = metrics.recall_score(y_val, y_pred, average = 'macro')\n",
        "f1_score = metrics.f1_score(y_val, y_pred,average='macro')\n",
        "metrics_data = {'classifier': ['NB'], 'model':['bool_bow'],\n",
        "    'accuracy': [accuracy], 'precision':[precision], 'recall':[recall], 'f1_score': [f1_score]}\n",
        "df = pd.DataFrame(data = metrics_data)\n",
        "metrics_df = pd.concat([metrics_df, df], axis = 1)\n",
        "clfs.append(clf)"
      ],
      "id": "competent-kentucky",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT4Vc8JmiHkh"
      },
      "source": [
        ""
      ],
      "id": "yT4Vc8JmiHkh",
      "execution_count": null,
      "outputs": []
    }
  ]
}